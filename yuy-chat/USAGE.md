# yuy-chat - GuÃ­a de Uso Completa

## ğŸ“– Contenido

1. [InstalaciÃ³n](#instalaciÃ³n)
2. [Primera Vez](#primera-vez)
3. [Uso Diario](#uso-diario)
4. [ConfiguraciÃ³n Avanzada](#configuraciÃ³n-avanzada)
5. [IntegraciÃ³n con HuggingFace](#integraciÃ³n-con-huggingface)
6. [Tips y Trucos](#tips-y-trucos)
7. [Troubleshooting](#troubleshooting)

---

## ğŸ”§ InstalaciÃ³n

### Termux (Android)

```bash
# Instalar Rust
pkg install rust

# Clonar y compilar
git clone https://github.com/YuuKi-OS/yuy-chat
cd yuy-chat
cargo build --release -j 1  # Usar 1 thread en Termux

# Instalar globalmente
cargo install --path .
```

### Linux/macOS

```bash
# Clonar y compilar
git clone https://github.com/YuuKi-OS/yuy-chat
cd yuy-chat
cargo build --release

# Instalar
cargo install --path .
```

### Windows

```bash
# Mismo proceso que Linux/macOS
git clone https://github.com/YuuKi-OS/yuy-chat
cd yuy-chat
cargo build --release
cargo install --path .
```

---

## ğŸ¬ Primera Vez

### 1. AsegÃºrate de tener modelos

yuy-chat busca modelos en `~/.yuuki/models/` por defecto.

**OpciÃ³n A: Usar yuy**
```bash
yuy download Yuuki-best
```

**OpciÃ³n B: Copiar manualmente**
```bash
mkdir -p ~/.yuuki/models/
cp /path/to/your/model.gguf ~/.yuuki/models/
```

### 2. Instalar llama.cpp

**Termux:**
```bash
pkg install llama-cpp
```

**macOS:**
```bash
brew install llama.cpp
```

**Linux:**
```bash
# Descargar desde releases
wget https://github.com/ggerganov/llama.cpp/releases/...
chmod +x llama-cli
sudo mv llama-cli /usr/local/bin/
```

### 3. Ejecutar yuy-chat

```bash
yuy-chat
```

VerÃ¡s el selector de modelos. Usa `â†‘/â†“` para navegar y `Enter` para seleccionar.

---

## ğŸ’¬ Uso Diario

### Flujo BÃ¡sico

```
1. Ejecuta: yuy-chat
   â†“
2. Selecciona modelo con â†‘/â†“ y Enter
   â†“
3. Escribe tu mensaje
   â†“
4. Presiona Enter para enviar
   â†“
5. Yuuki responde (streaming)
   â†“
6. ContinÃºa la conversaciÃ³n
```

### Atajos de Teclado Ãštiles

**En chat:**
- `Enter` - Enviar mensaje
- `Shift+Enter` - Nueva lÃ­nea (para mensajes multi-lÃ­nea)
- `Ctrl+L` - Limpiar chat
- `Ctrl+S` - Guardar conversaciÃ³n
- `Ctrl+C` - Abrir menÃº

**Escribir cÃ³digo:**
```
You: Dame un ejemplo de cÃ³digo Python

[Shift+Enter para nueva lÃ­nea]
def hello():
    print("Hola")
[Shift+Enter]

hello()

[Ctrl+Enter para enviar]
```

### Cambiar Preset

```
1. Ctrl+C (abrir menÃº)
   â†“
2. Presiona 2 (Change Preset)
   â†“
   Cicla entre: Creative â†’ Balanced â†’ Precise
```

**CuÃ¡ndo usar cada preset:**
- **Creative**: Escribir historias, brainstorming, ideas
- **Balanced**: Uso general, conversaciÃ³n
- **Precise**: CÃ³digo, matemÃ¡ticas, datos exactos

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Cambiar Directorio de Modelos

**MÃ©todo 1: ConfiguraciÃ³n**
```bash
yuy-chat
Ctrl+C â†’ 6 (Settings)
Editar "Models Directory"
```

**MÃ©todo 2: Archivo config**
```bash
nano ~/.config/yuy-chat/config.toml
```

```toml
models_dir = "/custom/path/to/models"
```

### Personalizar Presets

Edita el cÃ³digo o usa parÃ¡metros de llama.cpp directamente:

```bash
# En models/runtime.rs, modifica:
pub fn temperature(&self) -> f32 {
    match self {
        Preset::Creative => 0.9,  // MÃ¡s aleatorio
        // ...
    }
}
```

### Tema Claro

```toml
theme = "Light"
```

---

## ğŸŒ IntegraciÃ³n con HuggingFace

### 1. Obtener Token

1. Ve a https://huggingface.co/settings/tokens
2. Click "Create new token"
3. Tipo: "Read"
4. Copia el token

### 2. Configurar en yuy-chat

**MÃ©todo A: UI**
```
Ctrl+C â†’ 6 (Settings)
Navigate to "HuggingFace Token"
Enter â†’ Pega tu token
```

**MÃ©todo B: Config file**
```toml
hf_token = "hf_abcdefghijklmnopqrstuvwxyz1234567890"
```

### 3. Usar Modelos de HF

DespuÃ©s de configurar el token:
```
yuy-chat
[VerÃ¡s modelos locales + modelos HF API]

> Yuuki-best.gguf (Local)
  Yuuki-3.7.gguf (Local)  
  Yuuki-best (HF API) <-- Usa la API
```

**Ventajas:**
- No ocupa espacio local
- Siempre actualizado
- Acceso a modelos privados

**Desventajas:**
- Requiere internet
- MÃ¡s lento que local
- Rate limits en plan gratis

---

## ğŸ’¡ Tips y Trucos

### Guardar Conversaciones Importantes

```
Ctrl+S mientras chateas
â†’ Se guarda en ~/.config/yuy-chat/conversations/
```

### Cargar ConversaciÃ³n Anterior

```
Ctrl+C â†’ 4 (Load Conversation)
â†‘/â†“ para navegar
Enter para cargar
```

### Prompt Engineering

**Para mejores respuestas, sÃ© especÃ­fico:**

âŒ Malo:
```
You: Explica Rust
```

âœ… Bueno:
```
You: Explica el sistema de ownership en Rust con un ejemplo simple de borrowing. Quiero entender por quÃ© evita memory leaks.
```

### Conversaciones Multi-paso

```
You: Vamos a diseÃ±ar una API REST

Yuuki: Claro, Â¿quÃ© tipo de API?

You: Para gestionar tareas tipo TODO

Yuuki: Perfecto, estos son los endpoints...
```

### Usar Presets DinÃ¡micamente

- **Creative preset**: "Escribe un cuento de terror"
- **Precise preset**: "Â¿CuÃ¡l es la complejidad de quicksort?"
- **Balanced preset**: "ExplÃ­came cÃ³mo funciona Git"

---

## ğŸ”§ Troubleshooting

### Error: "No models found"

**SoluciÃ³n:**
```bash
# Verifica que tienes modelos
ls ~/.yuuki/models/

# Si estÃ¡ vacÃ­o, descarga uno
yuy download Yuuki-best

# O especifica otro directorio
yuy-chat --models-dir /path/to/models
```

### Error: "llama.cpp binary not found"

**SoluciÃ³n:**
```bash
# Termux
pkg install llama-cpp

# macOS
brew install llama.cpp

# Linux - verifica que estÃ¡ en PATH
which llama-cli
# Si no, instala o agrega al PATH
export PATH=$PATH:/path/to/llama-cpp
```

### Error: "Permission denied" (llamafile)

**SoluciÃ³n:**
```bash
chmod +x ~/.yuuki/models/*.llamafile
```

### Chat no responde / se congela

**DiagnÃ³stico:**
1. Verifica que llama.cpp funciona:
```bash
llama-cli -m ~/.yuuki/models/Yuuki-best.gguf -p "Hola"
```

2. Revisa logs:
```bash
RUST_LOG=debug yuy-chat
```

3. Reduce context size si es falta de RAM

### Respuestas muy lentas

**Causas comunes:**
- Modelo muy grande para tu RAM
- CuantizaciÃ³n muy alta (F32, Q8)
- Sin aceleraciÃ³n GPU

**SoluciÃ³n:**
```bash
# Descarga versiÃ³n cuantizada mÃ¡s pequeÃ±a
yuy download Yuuki-best --quant q4_0

# Verifica RAM disponible
free -h  # Linux
top      # macOS/Linux
```

### No puedo escribir mensajes largos

El input box tiene lÃ­mite visual pero **no de contenido**:
- Usa `Shift+Enter` para multi-lÃ­nea
- Scroll automÃ¡tico despuÃ©s de 5 lÃ­neas
- O escribe en editor externo y pega

### HuggingFace API no funciona

**Verifica:**
```bash
# Test manual
curl https://api-inference.huggingface.co/models/OpceanAI/Yuuki-best \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -d '{"inputs": "test"}'
```

**Problemas comunes:**
- Token expirado â†’ Genera nuevo
- Rate limit â†’ Espera o upgrade plan
- Modelo privado â†’ Verifica permisos

---

## ğŸ“Š Performance Tips

### Termux/MÃ³vil

```bash
# Usa modelos pequeÃ±os
yuy download Yuuki-best --quant q4_0

# Preset Balanced o Precise
# Creative es mÃ¡s lento
```

### Desktop High-end

```bash
# Usa Q8 o F32 para mejor calidad
yuy download Yuuki-best --quant q8_0

# Habilita GPU en llama.cpp
llama-cli -m model.gguf -ngl 32  # 32 layers en GPU
```

---

## ğŸ“ Casos de Uso

### 1. Coding Assistant

```
Preset: Precise

You: CÃ³mo implemento un servidor HTTP en Rust?
You: Muestra ejemplo con tokio
You: Agrega manejo de errores
You: Ahora agrega logging
```

### 2. Creative Writing

```
Preset: Creative

You: Escribe el inicio de una novela de ciencia ficciÃ³n ambientada en Marte en el aÃ±o 2157
You: ContinÃºa describiendo al protagonista
You: Â¿QuÃ© conflicto enfrenta?
```

### 3. Learning/Study

```
Preset: Balanced

You: ExplÃ­came la diferencia entre mutex y semaphore
You: Dame un ejemplo de cuÃ¡ndo usar cada uno
You: Â¿QuÃ© pasa si no uso sincronizaciÃ³n?
```

---

## ğŸš€ Workflow Recomendado

### Developer

```bash
# MaÃ±ana: Coding
yuy-chat  # Preset: Precise
> Ayuda con bugs, arquitectura, cÃ³digo

# Tarde: Docs
yuy-chat  # Preset: Balanced
> Escribir documentaciÃ³n, READMEs

# Noche: Ideas
yuy-chat  # Preset: Creative
> Brainstorming features
```

### Writer

```bash
yuy-chat  # Preset: Creative
> Generar ideas
> Escribir borradores
> Feedback de historias
```

### Estudiante

```bash
yuy-chat  # Preset: Balanced
> Explicaciones de conceptos
> Resolver dudas
> Preparar exÃ¡menes
```

---

**Â¿Preguntas? Abre un issue en GitHub!**

ğŸŒ¸ Hecho con amor por el equipo Yuuki
